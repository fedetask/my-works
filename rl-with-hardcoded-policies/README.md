# Reinforcement Learning with Hardcoded Policies
*Federico Taschin*

In this work I investigate how Reinforcement Learning can exploit hardcoded policies to learn faster,
and how hardcoded policies can be improved with Renforcement Learning. In this project, I explore the
following areas:

- **Behavior Trees** to create robust, modular, and efficient hardcoded policies
- **Off-policy RL** to learn RL policies when most of the experience is collected under hardcoded policies
- **Hierarchical RL** and the **Options framework** to exploit the hardcoded policies as sub-policies
and I propose what I call **RL with Backbone Policy**, i.e. the use of an hardcoded policy as a *backbone*
and the Reinforcement Learning to learn to correct it and improve on it.

One of the project goals was initally to participate in the [MineRL competition](https://minerl.io/competition/)
but due to the breadth of the scope of my research and the lack of sufficient time, I could not achieve it.

However, this research project has been **extremely useful** to me as it greatly exposed me to Hierarchical RL 
literature, which I intend to work on in my near future. Most importantly, it was an important lesson on how to
structure a reseach project and what errors to avoid. This failure has probably been **much more useful** than
many successes! 

Read the [full report here](report.pdf)
